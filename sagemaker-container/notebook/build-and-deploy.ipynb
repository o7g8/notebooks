{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multi Model Server Container for Differential Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter install\n",
    "sudo /opt/conda/bin/conda install ipython jupyter \n",
    "/opt/conda/bin/jupyter notebook --ip=0.0.0.0 --port=8080 --no-browser\n",
    "\n",
    "This notebook demonstrates how to build and use a custom Docker container for serving with Amazon SageMaker that leverages on the <strong>Multi Model Server (MMS)</strong> and <strong>sagemaker-inference-toolkit</strong> libraries for serving models through Amazon SageMaker's endpoints.\n",
    "We will also see how MMS allows deploying multiple models on a single endpoint thanks to the multi-model endpoints functionality of Amazon SageMaker Hosting (https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html).\n",
    "\n",
    "Useful links:\n",
    "- https://github.com/awslabs/multi-model-server/\n",
    "- https://github.com/aws/sagemaker-inference-toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining some variables like the current execution role, the ECR repository that we are going to use for pushing the custom Docker container and a default Amazon S3 bucket to be used by Amazon SageMaker.\n",
    "\n",
    "The whole deployment takes 10-12min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785577973223\n",
      "us-east-1\n",
      "arn:aws:iam::785577973223:role/service-role/AmazonSageMaker-ExecutionRole-20210715T110490\n",
      "sagemaker-us-east-1-785577973223\n",
      "sagemaker-serving-containers/diffdl-container-gpu\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "isGPU = True\n",
    "ecr_namespace = 'sagemaker-serving-containers/'\n",
    "prefix = 'diffdl-container'\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix + ('-gpu' if isGPU else '')\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(account_id)\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)\n",
    "print(ecr_repository_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build and push the container</h2>\n",
    "We are now ready to build this container and push it to Amazon ECR. This task is executed using a shell script stored in the ../script/ folder. Let's take a look at this script and then execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script builds the Docker container, then creates the repository if it does not exist, and finally pushes the container to the ECR repository. The build task requires a few minutes to be executed the first time, then Docker caches build outputs to be reused for the subsequent build operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing src/multi_model_serving.egg-info/PKG-INFO\n",
      "writing dependency_links to src/multi_model_serving.egg-info/dependency_links.txt\n",
      "writing top-level names to src/multi_model_serving.egg-info/top_level.txt\n",
      "reading manifest file 'src/multi_model_serving.egg-info/SOURCES.txt'\n",
      "writing manifest file 'src/multi_model_serving.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "creating multi_model_serving-1.0.0\n",
      "creating multi_model_serving-1.0.0/src\n",
      "creating multi_model_serving-1.0.0/src/multi_model_serving\n",
      "creating multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "copying files to multi_model_serving-1.0.0...\n",
      "copying setup.py -> multi_model_serving-1.0.0\n",
      "copying src/multi_model_serving/__init__.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/blackscholes.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/default_inference_handler.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/encoder.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/handler_service.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/predictor.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/test.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/xgb_content_types.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving.egg-info/PKG-INFO -> multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "copying src/multi_model_serving.egg-info/SOURCES.txt -> multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "copying src/multi_model_serving.egg-info/dependency_links.txt -> multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "copying src/multi_model_serving.egg-info/top_level.txt -> multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "Writing multi_model_serving-1.0.0/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'multi_model_serving-1.0.0' (and everything under it)\n",
      "Building with Dockerfile.gpu...\n",
      "../scripts/build_and_push.sh: line 19: docker: command not found\n"
     ]
    }
   ],
   "source": [
    "! ../scripts/build_and_push.sh $account_id $region $ecr_repository_name $isGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deploy with Amazon SageMaker</h2>\n",
    "\n",
    "\n",
    "<h3>Get the container URI</h3>\n",
    "Once we have correctly pushed our container to Amazon ECR, we are ready to deploy with Amazon SageMaker, which requires the ECR path to the Docker container used for serving as parameter for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785577973223.dkr.ecr.us-east-1.amazonaws.com/sagemaker-serving-containers/diffdl-container-gpu:latest\n"
     ]
    }
   ],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the image locally `docker run -it -p 3030:8080 --rm 785577973223.dkr.ecr.us-east-1.amazonaws.com/sagemaker-serving-containers/diffdl-container:latest`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prepare two models</h3>\n",
    "\n",
    "We are going to deploy two different XGBoost models to our model server. We will need the serialized models and the inference scripts that we want to use.\n",
    "We will store them in the current notebook folder, under <strong>model_and_code_1/</strong> and <strong>model_and_code_2/</strong>.\n",
    "\n",
    "The purpose of using different models is to show that you can also deploy models that require diverse features and pre/post processing code.\n",
    "\n",
    "First model is a regression model trained on the [Abalone data](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html) originally from the [UCI data repository](https://archive.ics.uci.edu/ml/datasets/abalone).\n",
    "For further information, please refer to this [example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb).\n",
    "\n",
    "Second model is a binary classification model built by following this workshop: https://github.com/aws-samples/amazon-sagemaker-build-train-deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./xgboost-model\n",
      "./code/\n",
      "./code/predictor.py\n",
      "./\n",
      "./model.bin\n",
      "./code/\n",
      "./code/predictor.py\n"
     ]
    }
   ],
   "source": [
    "! rm -rf ./model_and_code_1/.ipynb_checkpoints\n",
    "! rm -rf ./model_and_code_1/code/.ipynb_checkpoints\n",
    "! rm -rf ./model_and_code_2/.ipynb_checkpoints\n",
    "! rm -rf ./model_and_code_2/code/.ipynb_checkpoints\n",
    "\n",
    "! tar -C ./model_and_code_1/ -cvzf model1.tar.gz ./\n",
    "! tar -C ./model_and_code_2/ -cvzf model2.tar.gz ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deploy multiple models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model1.tar.gz to s3://sagemaker-us-east-1-785577973223/diffdl-container/modeldata/model1.tar.gz\n",
      "upload: ./model2.tar.gz to s3://sagemaker-us-east-1-785577973223/diffdl-container/modeldata/model2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data_prefix = 's3://{0}/{1}/modeldata'.format(bucket, prefix)\n",
    "\n",
    "s3_model_1_path = model_data_prefix + '/model1.tar.gz'\n",
    "!aws s3 cp model1.tar.gz {s3_model_1_path}\n",
    "s3_model_2_path = model_data_prefix + '/model2.tar.gz'\n",
    "!aws s3 cp model2.tar.gz {s3_model_2_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = 'multi-model-server-multidatamodel-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "model = Model(name = model_name,\n",
    "              model_data = '',\n",
    "              image_uri = container_image_uri,\n",
    "              role=role,\n",
    "              env = {\n",
    "                  'SAGEMAKER_PROGRAM': 'predictor'\n",
    "              },\n",
    "              #predictor_cls = sagemaker.predictor.Predictor,\n",
    "              sagemaker_session=sagemaker_session)\n",
    "\n",
    "multi_model = MultiDataModel(name = model_name,\n",
    "                             model_data_prefix = model_data_prefix,\n",
    "                             model = model,\n",
    "                             sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Note:</strong> the environment variable SAGEMAKER_PREDICTOR is used to specify the name of the custom inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-model-server-ep-2021-07-20-12-02-45-gpu\n",
      "ml.g4dn.xlarge\n",
      "-------"
     ]
    }
   ],
   "source": [
    "multi_endpoint_name = 'multi-model-server-ep-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "multi_endpoint_name = multi_endpoint_name + ('-gpu' if isGPU else '')\n",
    "print(multi_endpoint_name)\n",
    "\n",
    "# ml.m5.xlarge = 4vCPU/16GB; ml.g4dn.xlarge = 4vCPU/16GB.\n",
    "instance = 'ml.g4dn.xlarge' if isGPU else 'ml.m5.xlarge'\n",
    "print(instance)\n",
    "# GPU instances can't be used for multi-model endpoints! \n",
    "model_to_deploy = model if isGPU else multi_model\n",
    "pred = model_to_deploy.deploy(initial_instance_count=1,\n",
    "                          instance_type=instance,\n",
    "                          endpoint_name=multi_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Executing inferences</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Go to `scripts` and execute `python3 test_endpoint.py -e multi-model-server-ep-2021-07-16-17-28-36`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md(\"Go to `scripts` and execute `python3 test_endpoint.py -e {}`\".format(multi_endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the multi-model endpoint is ready, we can invoke either model1 or model2 by changing the target_model variable in the predict() function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "pred = Predictor(multi_endpoint_name)\n",
    "pred.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"No module named 'xgboost'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/sagemaker_inference/transformer.py\", line 110, in transform\n    self.validate_and_initialize(model_dir=model_dir)\n  File \"/usr/local/lib/python3.6/dist-packages/sagemaker_inference/transformer.py\", line 158, in validate_and_initialize\n    self._model = self._model_fn(model_dir)\n  File \"/opt/ml/models/e4c005b7fa0c40203fa9b3bbcf0b28cf/model/code/predictor.py\", line 6, in model_fn\n    model = pkl.load(open(model_file, 'rb'))\nModuleNotFoundError: No module named 'xgboost'\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/multi-model-server-ep-2021-07-16-14-25-28 in account 785577973223 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-62ba0c9e2cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'77,33,143.0,101,212.2,102,104.9,120,15.3,4,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_archive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/model1.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_archive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#pred.predict(item)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"No module named 'xgboost'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/sagemaker_inference/transformer.py\", line 110, in transform\n    self.validate_and_initialize(model_dir=model_dir)\n  File \"/usr/local/lib/python3.6/dist-packages/sagemaker_inference/transformer.py\", line 158, in validate_and_initialize\n    self._model = self._model_fn(model_dir)\n  File \"/opt/ml/models/e4c005b7fa0c40203fa9b3bbcf0b28cf/model/code/predictor.py\", line 6, in model_fn\n    model = pkl.load(open(model_file, 'rb'))\nModuleNotFoundError: No module named 'xgboost'\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/multi-model-server-ep-2021-07-16-14-25-28 in account 785577973223 for more information."
     ]
    }
   ],
   "source": [
    "item = '77,33,143.0,101,212.2,102,104.9,120,15.3,4,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1'\n",
    "model_archive = '/model1.tar.gz'\n",
    "pred.predict(item, target_model=model_archive)\n",
    "#pred.predict(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = '0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,73.0,79.0,32.0,27.0,45.0,48.0,13.0,62.0'\n",
    "model_archive = '/model2.tar.gz'\n",
    "pred.predict(item, target_model=model_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.delete_endpoint()\n",
    "pred.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
