{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multi Model Server Container for Differential Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build and use a custom Docker container for serving with Amazon SageMaker that leverages on the <strong>Multi Model Server (MMS)</strong> and <strong>sagemaker-inference-toolkit</strong> libraries for serving models through Amazon SageMaker's endpoints.\n",
    "We will also see how MMS allows deploying multiple models on a single endpoint thanks to the multi-model endpoints functionality of Amazon SageMaker Hosting (https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html).\n",
    "\n",
    "Useful links:\n",
    "- https://github.com/awslabs/multi-model-server/\n",
    "- https://github.com/aws/sagemaker-inference-toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining some variables like the current execution role, the ECR repository that we are going to use for pushing the custom Docker container and a default Amazon S3 bucket to be used by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785577973223\n",
      "us-east-1\n",
      "arn:aws:iam::785577973223:role/service-role/AmazonSageMaker-ExecutionRole-20210715T110490\n",
      "sagemaker-us-east-1-785577973223\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "ecr_namespace = 'sagemaker-serving-containers/'\n",
    "prefix = 'diffdl-container'\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(account_id)\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the Dockerfile which defines the statements for building our serving container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33mtensorflow/tensorflow:latest\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mLABEL\u001b[39;49;00m \u001b[31mmaintainer\u001b[39;49;00m=\u001b[33m\"Oleg Grytsynevych\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Set a docker label to advertise multi-model support on the container\u001b[39;49;00m\n",
      "\u001b[34mLABEL\u001b[39;49;00m com.amazonaws.sagemaker.capabilities.multi-models=true\n",
      "\u001b[37m# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\u001b[39;49;00m\n",
      "\u001b[34mLABEL\u001b[39;49;00m com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
      "\n",
      "\u001b[37m# Install python and other runtime dependencies\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update && \u001b[33m\\\u001b[39;49;00m\n",
      "    apt-get -y install \u001b[33m\\\u001b[39;49;00m\n",
      "        build-essential \u001b[33m\\\u001b[39;49;00m\n",
      "        libatlas-dev \u001b[33m\\\u001b[39;49;00m\n",
      "        git \u001b[33m\\\u001b[39;49;00m\n",
      "        wget \u001b[33m\\\u001b[39;49;00m\n",
      "        curl \u001b[33m\\\u001b[39;49;00m\n",
      "        openjdk-8-jdk-headless\n",
      "\n",
      "\u001b[37m# Python wonâ€™t try to write .pyc or .pyo files on the import of source modules\u001b[39;49;00m\n",
      "\u001b[37m# Force stdin, stdout and stderr to be totally unbuffered. Good for logging\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mPYTHONDONTWRITEBYTECODE\u001b[39;49;00m=\u001b[34m1\u001b[39;49;00m \u001b[31mPYTHONUNBUFFERED\u001b[39;49;00m=\u001b[34m1\u001b[39;49;00m \u001b[31mPYTHONIOENCODING\u001b[39;49;00m=UTF-8 \u001b[31mLANG\u001b[39;49;00m=C.UTF-8 \u001b[31mLC_ALL\u001b[39;49;00m=C.UTF-8\n",
      "\n",
      "\u001b[37m# Install MMS, and SageMaker Inference Toolkit to set up MMS\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip --no-cache-dir install multi-model-server \u001b[33m\\\u001b[39;49;00m\n",
      "                               sagemaker-inference \u001b[33m\\\u001b[39;49;00m\n",
      "                               retrying\n",
      "                                \n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m code/multi_model_serving-1.0.0.tar.gz /multi_model_serving-1.0.0.tar.gz\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install /multi_model_serving-1.0.0.tar.gz && rm /multi_model_serving-1.0.0.tar.gz\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m code/serve.py /serve.py\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"python\"\u001b[39;49;00m, \u001b[33m\"serve.py\"\u001b[39;49;00m]\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ../docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At high-level the Dockerfile specifies the following operations for building this container:\n",
    "\n",
    "- Set two Docker labels to advertise multi-model support and to enable the container using the SAGEMAKER_BIND_TO_PORT environment variable, if present\n",
    "- Install libraries (including OpenJDK since MMS frontend is Java-based) and Python 3.6 through miniconda\n",
    "- Set e few environment variables, including PYTHONUNBUFFERED which is used to avoid buffering Python standard output (useful for logging)\n",
    "- Install XGBoost (it is the ML framework of choice for this example)\n",
    "- Install Multi Model Server (MMS) and SageMaker Inference Toolkit\n",
    "- Copy a .tar.gz package named <strong>multi_model_serving-1.0.0.tar.gz</strong> in the WORKDIR\n",
    "- Install this package\n",
    "- Copy the serve.py file in the WORKDIR and use it as the Docker ENTRYPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the content of the <strong>serve.py</strong> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m absolute_import\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m CalledProcessError\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mretrying\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m retry\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmulti_model_serving\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m handler_service\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_inference\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m model_server\n",
      "\n",
      "HANDLER_SERVICE = handler_service.\u001b[31m__name__\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_retry_if_error\u001b[39;49;00m(exception):\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(exception, CalledProcessError)\n",
      "\n",
      "\u001b[90m@retry\u001b[39;49;00m(stop_max_delay=\u001b[34m1000\u001b[39;49;00m * \u001b[34m30\u001b[39;49;00m,\n",
      "       retry_on_exception=_retry_if_error)\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_start_model_server\u001b[39;49;00m():\n",
      "    \u001b[37m# there's a race condition that causes the model server command to\u001b[39;49;00m\n",
      "    \u001b[37m# sometimes fail with 'bad address'. more investigation needed\u001b[39;49;00m\n",
      "    \u001b[37m# retry starting mms until it's ready\u001b[39;49;00m\n",
      "    model_server.start_model_server(handler_service=HANDLER_SERVICE)\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    _start_model_server()\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ../docker/code/serve.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Handler Service</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the Dockerfile above, you might be askiong yourself what the <strong>multi_model_serving-1.0.0.tar.gz</strong> package is.\n",
    "When building a framework container for serving, sagemaker-inference-toolkit allows you to pass an handler service that will define the default inference handling logic, when users do not pass any custom inference script. The package above contains this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the content of the handler service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_inference\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdefault_handler_service\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DefaultHandlerService\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_inference\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransformer\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Transformer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmulti_model_serving\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdefault_inference_handler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DefaultDiffDLInferenceHandler\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\n",
      "ENABLE_MULTI_MODEL = os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mSAGEMAKER_MULTI_MODEL\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) == \u001b[33m\"\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mHandlerService\u001b[39;49;00m(DefaultHandlerService):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[36mself\u001b[39;49;00m._initialized = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        transformer = Transformer(default_inference_handler=DefaultDiffDLInferenceHandler())\n",
      "        \u001b[36msuper\u001b[39;49;00m(HandlerService, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m(transformer=transformer)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minitialize\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, context):\n",
      "        \u001b[37m# This code is a workaround to fix a bug in the inference toolkit not setting the\u001b[39;49;00m\n",
      "        \u001b[37m# user module path correctly when multi-model is enabled.\u001b[39;49;00m\n",
      "        \u001b[37m# To be removed when the toolkit fix is available.\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._initialized) \u001b[35mand\u001b[39;49;00m ENABLE_MULTI_MODEL:\n",
      "            code_dir = context.system_properties.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) + \u001b[33m'\u001b[39;49;00m\u001b[33m/code\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "            sys.path.append(code_dir)\n",
      "            \u001b[36mself\u001b[39;49;00m._initialized = \u001b[34mTrue\u001b[39;49;00m\n",
      "        \n",
      "        \u001b[37m# Printing system properties for debugging purposes.\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mContext system properties: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(context.system_properties)\n",
      "\n",
      "        \u001b[36msuper\u001b[39;49;00m().initialize(context)\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ../package/src/multi_model_serving/handler_service.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the logic defined in the default inference handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpkl\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_inference\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m content_types, decoder, default_inference_handler, encoder, errors\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmulti_model_serving\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m encoder \u001b[34mas\u001b[39;49;00m xgb_encoders\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDefaultDiffDLInferenceHandler\u001b[39;49;00m(default_inference_handler.DefaultInferenceHandler):\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdefault_input_fn\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_data, content_type):\n",
      "        \u001b[33m\"\"\"Take request data and de-serializes the data into an object for prediction.\u001b[39;49;00m\n",
      "\u001b[33m        When an InvokeEndpoint operation is made against an Endpoint running SageMaker model server,\u001b[39;49;00m\n",
      "\u001b[33m        the model server receives two pieces of information:\u001b[39;49;00m\n",
      "\u001b[33m            - The request Content-Type, for example \"application/json\"\u001b[39;49;00m\n",
      "\u001b[33m            - The request data, which is at most 5 MB (5 * 1024 * 1024 bytes) in size.\u001b[39;49;00m\n",
      "\u001b[33m        The input_fn is responsible to take the request data and pre-process it before prediction.\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            input_data (obj): the request data.\u001b[39;49;00m\n",
      "\u001b[33m            content_type (str): the request Content-Type.\u001b[39;49;00m\n",
      "\u001b[33m        Returns:\u001b[39;49;00m\n",
      "\u001b[33m            (obj): data ready for prediction. For XGBoost, this defaults to DMatrix.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m xgb_encoders.decode(input_data, content_type)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdefault_predict_fn\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data, model):\n",
      "        \u001b[33m\"\"\"A default predict_fn for XGBooost Framework. Calls a model on data deserialized in input_fn.\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            input_data: input data (Numpy array) for prediction deserialized by input_fn\u001b[39;49;00m\n",
      "\u001b[33m            model: XGBoost model loaded in memory by model_fn\u001b[39;49;00m\n",
      "\u001b[33m        Returns: a prediction\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        output = model.predict(data, validate_features=\u001b[34mFalse\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m output\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdefault_output_fn\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, prediction, accept):\n",
      "        \u001b[33m\"\"\"A default output_fn for XGBoost. Serializes predictions from predict_fn to JSON, CSV or NPY format.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            prediction: a prediction result from predict_fn\u001b[39;49;00m\n",
      "\u001b[33m            accept: type which the output data needs to be serialized\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        Returns: output data serialized\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m encoder.encode(prediction, accept)\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ../package/src/multi_model_serving/default_inference_handler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build and push the container</h2>\n",
    "We are now ready to build this container and push it to Amazon ECR. This task is executed using a shell script stored in the ../script/ folder. Let's take a look at this script and then execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mACCOUNT_ID\u001b[39;49;00m=\u001b[31m$1\u001b[39;49;00m\n",
      "\u001b[31mREGION\u001b[39;49;00m=\u001b[31m$2\u001b[39;49;00m\n",
      "\u001b[31mREPO_NAME\u001b[39;49;00m=\u001b[31m$3\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mcd\u001b[39;49;00m ../package/ && python setup.py sdist && cp dist/multi_model_serving-1.0.0.tar.gz docker/code/\n",
      "\n",
      "docker build -f ../docker/Dockerfile -t \u001b[31m$REPO_NAME\u001b[39;49;00m ../docker\n",
      "\n",
      "docker tag \u001b[31m$REPO_NAME\u001b[39;49;00m \u001b[31m$ACCOUNT_ID\u001b[39;49;00m.dkr.ecr.\u001b[31m$REGION\u001b[39;49;00m.amazonaws.com/\u001b[31m$REPO_NAME\u001b[39;49;00m:latest\n",
      "\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --no-include-email --registry-ids \u001b[31m$ACCOUNT_ID\u001b[39;49;00m\u001b[34m)\u001b[39;49;00m\n",
      "\n",
      "aws ecr describe-repositories --repository-names \u001b[31m$REPO_NAME\u001b[39;49;00m || aws ecr create-repository --repository-name \u001b[31m$REPO_NAME\u001b[39;49;00m\n",
      "\n",
      "docker push \u001b[31m$ACCOUNT_ID\u001b[39;49;00m.dkr.ecr.\u001b[31m$REGION\u001b[39;49;00m.amazonaws.com/\u001b[31m$REPO_NAME\u001b[39;49;00m:latest\n"
     ]
    }
   ],
   "source": [
    "! pygmentize ../scripts/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>--------------------------------------------------------------------------------------------------------------------</h3>\n",
    "\n",
    "The script builds the Docker container, then creates the repository if it does not exist, and finally pushes the container to the ECR repository. The build task requires a few minutes to be executed the first time, then Docker caches build outputs to be reused for the subsequent build operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: sudo: not found\n"
     ]
    }
   ],
   "source": [
    "!sudo yum -y install docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing src/multi_model_serving.egg-info/PKG-INFO\n",
      "writing dependency_links to src/multi_model_serving.egg-info/dependency_links.txt\n",
      "writing top-level names to src/multi_model_serving.egg-info/top_level.txt\n",
      "reading manifest file 'src/multi_model_serving.egg-info/SOURCES.txt'\n",
      "writing manifest file 'src/multi_model_serving.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "creating multi_model_serving-1.0.0\n",
      "creating multi_model_serving-1.0.0/src\n",
      "creating multi_model_serving-1.0.0/src/multi_model_serving\n",
      "creating multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "copying files to multi_model_serving-1.0.0...\n",
      "copying setup.py -> multi_model_serving-1.0.0\n",
      "copying src/multi_model_serving/__init__.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/default_inference_handler.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/encoder.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/handler_service.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving/xgb_content_types.py -> multi_model_serving-1.0.0/src/multi_model_serving\n",
      "copying src/multi_model_serving.egg-info/PKG-INFO -> multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "copying src/multi_model_serving.egg-info/SOURCES.txt -> multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "copying src/multi_model_serving.egg-info/dependency_links.txt -> multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "copying src/multi_model_serving.egg-info/top_level.txt -> multi_model_serving-1.0.0/src/multi_model_serving.egg-info\n",
      "Writing multi_model_serving-1.0.0/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'multi_model_serving-1.0.0' (and everything under it)\n",
      "cp: cannot create regular file 'docker/code/': No such file or directory\n",
      "../scripts/build_and_push.sh: 8: ../scripts/build_and_push.sh: docker: not found\n",
      "../scripts/build_and_push.sh: 10: ../scripts/build_and_push.sh: docker: not found\n",
      "../scripts/build_and_push.sh: 12: ../scripts/build_and_push.sh: docker: not found\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-east-1:785577973223:repository/sagemaker-serving-containers/diffdl-container\",\n",
      "            \"registryId\": \"785577973223\",\n",
      "            \"repositoryName\": \"sagemaker-serving-containers/diffdl-container\",\n",
      "            \"repositoryUri\": \"785577973223.dkr.ecr.us-east-1.amazonaws.com/sagemaker-serving-containers/diffdl-container\",\n",
      "            \"createdAt\": 1626372297.0,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "../scripts/build_and_push.sh: 16: ../scripts/build_and_push.sh: docker: not found\n"
     ]
    }
   ],
   "source": [
    "! ../scripts/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deploy with Amazon SageMaker</h2>\n",
    "\n",
    "\n",
    "<h3>Get the container URI</h3>\n",
    "Once we have correctly pushed our container to Amazon ECR, we are ready to deploy with Amazon SageMaker, which requires the ECR path to the Docker container used for serving as parameter for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785577973223.dkr.ecr.us-east-1.amazonaws.com/sagemaker-serving-containers/multi-model-server-container:latest\n"
     ]
    }
   ],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prepare two models</h3>\n",
    "\n",
    "We are going to deploy two different XGBoost models to our model server. We will need the serialized models and the inference scripts that we want to use.\n",
    "We will store them in the current notebook folder, under <strong>model_and_code_1/</strong> and <strong>model_and_code_2/</strong>.\n",
    "\n",
    "The purpose of using different models is to show that you can also deploy models that require diverse features and pre/post processing code.\n",
    "\n",
    "First model is a regression model trained on the [Abalone data](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html) originally from the [UCI data repository](https://archive.ics.uci.edu/ml/datasets/abalone).\n",
    "For further information, please refer to this [example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb).\n",
    "\n",
    "Second model is a binary classification model built by following this workshop: https://github.com/aws-samples/amazon-sagemaker-build-train-deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./xgboost-model\n",
      "./code/\n",
      "./code/predictor.py\n",
      "./\n",
      "./model.bin\n",
      "./code/\n",
      "./code/predictor.py\n"
     ]
    }
   ],
   "source": [
    "! rm -rf ./model_and_code_1/.ipynb_checkpoints\n",
    "! rm -rf ./model_and_code_1/code/.ipynb_checkpoints\n",
    "! rm -rf ./model_and_code_2/.ipynb_checkpoints\n",
    "! rm -rf ./model_and_code_2/code/.ipynb_checkpoints\n",
    "\n",
    "! tar -C ./model_and_code_1/ -cvzf model1.tar.gz ./\n",
    "! tar -C ./model_and_code_2/ -cvzf model2.tar.gz ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the custom inference script for the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpkl\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    model_file = model_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/xgboost-model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    model = pkl.load(\u001b[36mopen\u001b[39;49;00m(model_file, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n"
     ]
    }
   ],
   "source": [
    "! pygmentize model_and_code_1/code/predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the one for the second model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpkl\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmulti_model_serving\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m encoder \u001b[34mas\u001b[39;49;00m xgb_encoders\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(input_data, content_type):    \n",
      "    \u001b[34mreturn\u001b[39;49;00m xgb_encoders.decode(input_data, content_type)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    model_file = model_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    model = pkl.load(\u001b[36mopen\u001b[39;49;00m(model_file, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n"
     ]
    }
   ],
   "source": [
    "! pygmentize model_and_code_2/code/predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deploy a single model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path = 's3://{0}/{1}/model/model1.tar.gz'.format(bucket, prefix)\n",
    "!aws s3 cp model1.tar.gz {s3_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sagemaker-custom-serving-containers/multi-model-server-container/notebook\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = 'multi-model-server-model-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "model = Model(model_data = s3_model_path,\n",
    "              image_uri = container_image_uri,\n",
    "              env = {\n",
    "                  'SAGEMAKER_PROGRAM': 'predictor'\n",
    "              },\n",
    "              role=role,\n",
    "              name = model_name,\n",
    "              predictor_cls = sagemaker.predictor.Predictor,\n",
    "              #sagemaker_session=sagemaker_session #comment this line for local mode.\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Note:</strong> the environment variable SAGEMAKER_PREDICTOR is used to specify the name of the custom inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-model-server-single-ep-2021-07-15-16-41-10\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docker': 'docker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ed58b465d7b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m pred = model.deploy(initial_instance_count=1,\n\u001b[1;32m      4\u001b[0m                     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     endpoint_name=endpoint_name)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m         )\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m         self.sagemaker_client.create_endpoint(\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         )\n\u001b[1;32m   2981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, EndpointName, EndpointConfigName, Tags)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalEndpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m         )\n\u001b[1;32m    577\u001b[0m         self.container.serve(\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ModelDataUrl\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Environment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         )\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self, model_dir, environment)\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0menvironment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDIR_PARAM_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/opt/ml/code\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_ecr_login_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0m_pull_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_ecr_login_if_needed\u001b[0;34m(boto_session, image)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;31m# do we have the image?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_check_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docker images -q %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_check_output\u001b[0;34m(cmd, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 411\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    798\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1549\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docker': 'docker'"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'multi-model-server-single-ep-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "pred = model.deploy(initial_instance_count=1,\n",
    "                    instance_type='local',\n",
    "                    endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "pred.serializer = sagemaker.serializers.CSVSerializer()\n",
    "item = '77,33,143.0,101,212.2,102,104.9,120,15.3,4,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1'\n",
    "pred.predict(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.delete_endpoint()\n",
    "pred.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deploy multiple models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_prefix = 's3://{0}/{1}/modeldata'.format(bucket, prefix)\n",
    "\n",
    "s3_model_1_path = model_data_prefix + '/model1.tar.gz'\n",
    "!aws s3 cp model1.tar.gz {s3_model_1_path}\n",
    "s3_model_2_path = model_data_prefix + '/model2.tar.gz'\n",
    "!aws s3 cp model2.tar.gz {s3_model_2_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = 'multi-model-server-multidatamodel-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "model = Model(name = model_name,\n",
    "              model_data = '',\n",
    "              image_uri = container_image_uri,\n",
    "              role=role,\n",
    "              env = {\n",
    "                  'SAGEMAKER_PROGRAM': 'predictor'\n",
    "              },\n",
    "              predictor_cls = sagemaker.predictor.Predictor,\n",
    "              sagemaker_session=sagemaker_session)\n",
    "\n",
    "multi_model = MultiDataModel(name = model_name,\n",
    "                             model_data_prefix = model_data_prefix,\n",
    "                             model = model,\n",
    "                             sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Note:</strong> the environment variable SAGEMAKER_PREDICTOR is used to specify the name of the custom inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_endpoint_name = 'multi-model-server-ep-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(multi_endpoint_name)\n",
    "\n",
    "pred = multi_model.deploy(initial_instance_count=1,\n",
    "                          instance_type='ml.m5.xlarge',\n",
    "                          endpoint_name=multi_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Executing inferences</h3>\n",
    "Once the multi-model endpoint is ready, we can invoke either model1 or model2 by changing the target_model variable in the predict() function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "pred = Predictor(multi_endpoint_name)\n",
    "pred.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = '77,33,143.0,101,212.2,102,104.9,120,15.3,4,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1'\n",
    "model_archive = '/model1.tar.gz'\n",
    "pred.predict(item, target_model=model_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = '0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,73.0,79.0,32.0,27.0,45.0,48.0,13.0,62.0'\n",
    "model_archive = '/model2.tar.gz'\n",
    "pred.predict(item, target_model=model_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.delete_endpoint()\n",
    "pred.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
